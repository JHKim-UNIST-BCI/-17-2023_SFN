{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Jaehun Kim\n",
    "# Email: rlawogns1204@unist.ac.kr\n",
    "# Affiliation: UNIST BME BCILAB\n",
    "# Date: 2023-05-24\n",
    "#\n",
    "# This code implements a tactile information processing model using a spiking\n",
    "# neural network (SNN). It simulates the processing of tactile information from\n",
    "# mechanoreceptors in the skin through primary afferent fibers (PA), cuneate nucleus\n",
    "# neurons (PN and IN), and ultimately, somatosensory cortex neurons. The model\n",
    "# incorporates lateral inhibition and various receptive field properties to\n",
    "# represent a realistic processing of touch stimuli.\n",
    "\n",
    "# with DIGIT-sensor\n",
    "# pip install torch==1.8.1+cu101 torchvision==0.9.1+cu101 torchaudio==0.8.1 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "import time\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import io\n",
    "import datetime\n",
    "\n",
    "from Function.ReceptiveField import *\n",
    "from Function.SNNModule import *\n",
    "from Function.plot_SNN import *\n",
    "from Function.SNN_DIGIT import *\n",
    "from Function.Stimulation_GPU import *\n",
    "import cv2\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True\n",
      "CUDA version: 10.1\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available and print the CUDA version\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "# Enable GPU support if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Set the device to 'cpu'\n",
    "device = 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "# Start of the SNN (Spiking Neural Network) code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set sensor dimensions (height and width in millimeters)\n",
    "sensor_h, sensor_w = 19, 16\n",
    "# Set pixel dimensions (number of pixels in height and width)\n",
    "pixel_h, pixel_w = 64, 48\n",
    "# Set image frames per second (FPS) of tactile sensor\n",
    "image_FPS = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SA_rf shape: torch.Size([88, 3072]) with height = 11 with width = 8\n",
      "RA_rf shape: torch.Size([130, 3072]) with height = 13 with width = 10\n",
      "SA_CN_PN_RF shape:  torch.Size([54, 88]) SA_CN_PN_step_height: 9 SA_CN_PN_step_width: 6\n",
      "SA_CN_IN_RF shape:  torch.Size([54, 88]) SA_CN_IN_step_height: 9 SA_CN_IN_step_width: 6\n",
      "RA_CN_PN_RF shape:  torch.Size([88, 130]) RA_CN_PN_step_height: 11 RA_CN_PN_step_width: 8\n",
      "RA_CN_IN_RF shape:  torch.Size([88, 130]) RA_CN_IN_step_height: 11 RA_CN_IN_step_width: 8\n",
      "SA_INtoPN_RF shape:  torch.Size([54, 54])\n",
      "RA_INtoPN_RF shape:  torch.Size([88, 88])\n",
      "CN_PN_SA_RF shape:  torch.Size([112, 54]) CN_PN_SA_RF_step_height: 7 CN_PN_SA_RF_step_width: 4\n",
      "CN_IN_SA_RF shape:  torch.Size([112, 54]) CN_IN_SA_RF_step_height: 7 CN_IN_SA_RF_step_width: 4\n",
      "CN_PN_RA_RF shape:  torch.Size([112, 88]) CN_PN_RA_RF_step_height: 7 CN_PN_RA_RF_step_width: 4\n",
      "CN_IN_RA_RF shape:  torch.Size([112, 88]) CN_IN_RA_RF_step_height: 7 CN_IN_RA_RF_step_width: 4\n",
      "CN_INtoPN_RF shape:  torch.Size([112, 112])\n"
     ]
    }
   ],
   "source": [
    "#1st layer\n",
    "############################################################################################################################################################\n",
    "# Generate Slowly Adapting (SA) and Rapidly Adapting (RA) receptive fields\n",
    "SA_RF, [SA_rf_height, SA_rf_width] = generate_mechanoreceptor_to_afferent_rf(kernel_w=9, kernel_h=11, step_size=5, device=device)\n",
    "RA_RF, [RA_rf_height, RA_rf_width] = generate_mechanoreceptor_to_afferent_rf(kernel_w=11, kernel_h=14, step_size=4, device=device)\n",
    "# Print the shape of the SA_rf variable\n",
    "print(\"SA_rf shape:\", SA_RF.shape, 'with height =',SA_rf_height, 'with width =', SA_rf_width)\n",
    "print(\"RA_rf shape:\", RA_RF.shape, 'with height =',RA_rf_height, 'with width =', RA_rf_width)\n",
    "############################################################################################################################################################\n",
    "\n",
    "#2nd layer\n",
    "############################################################################################################################################################\n",
    "# Define optimized receptive fields and synaptic delays\n",
    "CN_PN_RF = [torch.tensor([[0, 0, 0], [0, 1, 0], [0, 0, 0]],device=device) * 4]\n",
    "CN_IN_RF = [torch.tensor([[1, 1, 1], [1, 0, 1], [1, 1, 1]],device=device)]\n",
    "CN_SD = [torch.tensor([[2, 2, 2], [2, 2, 2], [2, 2, 2]], device=device)]\n",
    "\n",
    "CN_INtoPN_RF = []\n",
    "\n",
    "# Check if the sizes of the inner tensors are different and print the index\n",
    "for i, (PN, IN) in enumerate(zip(CN_PN_RF, CN_IN_RF)):\n",
    "    if PN.size() != IN.size():\n",
    "        raise ValueError(\n",
    "            f\"The inner tensors at index {i} have different sizes: {PN.size()} != {IN.size()}\")\n",
    "\n",
    "SA_CN_PN_RF, [SA_CN_PN_step_height, SA_CN_PN_step_width] = generate_weight(CN_PN_RF, pixel_h=SA_rf_height,pixel_w=SA_rf_width, step_size=1, device=device)\n",
    "SA_CN_IN_RF, [SA_CN_IN_step_height, SA_CN_IN_step_width] = generate_weight(CN_IN_RF, pixel_h=SA_rf_height,pixel_w=SA_rf_width, step_size=1, device=device)\n",
    "SA_CN_SD, [SA_CN_SD_step_height, SA_CN_SD_step_width]  = generate_weight(CN_SD, pixel_h=SA_rf_height,pixel_w=SA_rf_width, step_size=1, device=device)\n",
    "RA_CN_PN_RF, [RA_CN_PN_step_height, RA_CN_PN_step_width] = generate_weight(CN_PN_RF, pixel_h=RA_rf_height,pixel_w=RA_rf_width, step_size=1, device=device)\n",
    "RA_CN_IN_RF, [RA_CN_IN_step_height, RA_CN_IN_step_width] = generate_weight(CN_IN_RF, pixel_h=RA_rf_height,pixel_w=RA_rf_width, step_size=1, device=device)\n",
    "RA_CN_SD, [RA_CN_SD_step_height, RA_CN_SD_step_width] = generate_weight(CN_SD, pixel_h=RA_rf_height, pixel_w=RA_rf_width, step_size=1, device=device)\n",
    "\n",
    "SA_INtoPN_RF, SA_INtoPN_DN = create_weight_matrix(len(SA_CN_IN_RF), len(SA_CN_PN_RF), connection_probability=0.2, device=device)\n",
    "RA_INtoPN_RF, RA_INtoPN_DN = create_weight_matrix(len(RA_CN_IN_RF), len(RA_CN_PN_RF), connection_probability=0.2, device=device)\n",
    "\n",
    "print(\"SA_CN_PN_RF shape: \", SA_CN_PN_RF.shape,\"SA_CN_PN_step_height:\", SA_CN_PN_step_height,\"SA_CN_PN_step_width:\", SA_CN_PN_step_width)\n",
    "print(\"SA_CN_IN_RF shape: \", SA_CN_IN_RF.shape,\"SA_CN_IN_step_height:\", SA_CN_IN_step_height,\"SA_CN_IN_step_width:\", SA_CN_IN_step_width)\n",
    "print(\"RA_CN_PN_RF shape: \", RA_CN_PN_RF.shape,\"RA_CN_PN_step_height:\", RA_CN_PN_step_height,\"RA_CN_PN_step_width:\", RA_CN_PN_step_width)\n",
    "print(\"RA_CN_IN_RF shape: \", RA_CN_IN_RF.shape,\"RA_CN_IN_step_height:\", RA_CN_IN_step_height,\"RA_CN_IN_step_width:\", RA_CN_IN_step_width)\n",
    "print(\"SA_INtoPN_RF shape: \", SA_INtoPN_RF.shape)\n",
    "print(\"RA_INtoPN_RF shape: \", RA_INtoPN_RF.shape)\n",
    "############################################################################################################################################################\n",
    "\n",
    "#3rd layer\n",
    "############################################################################################################################################################\n",
    "# 3rd integration layer\n",
    "# Define optimized receptive fields and synaptic delays\n",
    "CN_PN_RF_set = [torch.tensor([[0, 0, 0], [0, 0, 0], [1, 1 ,1]], device=device),torch.tensor([[0, 0, 1], [0, 0, 1], [0, 0 ,1]], device=device),\n",
    "                torch.tensor([[1, 0, 0], [0, 1, 0], [0, 0, 1]], device=device),torch.tensor([[0, 0, 1], [0, 1, 0], [1, 0, 0]], device=device)]\n",
    "CN_IN_RF_set = [torch.tensor([[0, 0, 0], [1, 1 ,1], [0, 0 ,0]], device=device),torch.tensor([[0, 1, 0], [0, 1 ,0], [0, 1 ,0]], device=device),\n",
    "                torch.tensor([[0, 1, 0], [0, 0, 1], [0, 0, 0]], device=device)*3/2,torch.tensor([[0, 1, 0], [1, 0, 0], [0, 0, 0]], device=device)*3/2]\n",
    "CN_SD_set = [torch.tensor([[2, 2, 2], [2, 2, 2], [2, 2, 2]], device=device),torch.tensor([[2, 2, 2], [2, 2, 2], [2, 2, 2]], device=device),\n",
    "             torch.tensor([[2, 2, 2], [2, 2, 2], [2, 2, 2]], device=device),torch.tensor([[2, 2, 2], [2, 2, 2], [2, 2, 2]], device=device)]\n",
    "rf_sizes=[(7, 4)]\n",
    "\n",
    "CN_PN_RF_RA_set = [torch.tensor([[0, 0, 0, 0, 0], [0, 0, 0, 0, 0],[0, 0, 0, 0, 0],[1, 1, 1 ,1, 1], [1, 1, 1 ,1, 1]], device=device)/10*3,\n",
    "                   torch.tensor([[0, 0, 0, 1, 1], [0, 0, 0, 1, 1],[0, 0, 0, 1, 1],[0, 0, 0 ,1, 1], [0, 0, 0 ,1, 1]], device=device)/10*3,\n",
    "                   torch.tensor([[1, 0, 0, 0, 0], [0, 1, 0, 0, 0], [0, 0, 1, 0, 0], [0, 0, 0, 1, 0], [0, 0, 0, 0, 1]], device=device)/5*3,\n",
    "                   torch.tensor([[0, 0, 0, 0, 1], [0, 0, 0, 1, 0], [0, 0, 1, 0, 0], [0, 1, 0, 0, 0], [1, 0, 0, 0, 0]], device=device)/5*3]\n",
    "\n",
    "CN_IN_RF_RA_set = [torch.tensor([[0, 0, 0, 0, 0], [1, 1, 1 ,1, 1], [1, 1, 1 ,1, 1], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0]], device=device)/10*3,\n",
    "                   torch.tensor([[0, 1, 1, 0, 0], [0, 1, 1, 0, 0],[0, 1, 1, 0, 0],[0, 1, 1, 0, 0], [0, 1, 1, 0, 0]], device=device)/10*3,\n",
    "                   torch.tensor([[0, 1, 0, 0, 0], [0, 0, 1, 0, 0], [0, 0, 0, 1, 0], [0, 0, 0, 0, 1], [0, 0, 0, 0, 0]], device=device)/5*3,\n",
    "                   torch.tensor([[0, 0, 0, 1, 0], [0, 0, 1, 0, 0], [0, 1, 0, 0, 0], [1, 0, 0, 0, 0], [0, 0, 0, 0, 0]], device=device)/5*3,]\n",
    "\n",
    "CN_SD__RA_set = [torch.tensor([[2, 2, 2 ,2, 2],[2, 2, 2 ,2, 2],[2, 2, 2 ,2, 2],[2, 2, 2 ,2, 2],[2, 2, 2 ,2, 2]], device=device),\n",
    "                 torch.tensor([[2, 2, 2 ,2, 2],[2, 2, 2 ,2, 2],[2, 2, 2 ,2, 2],[2, 2, 2 ,2, 2],[2, 2, 2 ,2, 2]], device=device),\n",
    "                 torch.tensor([[2, 2, 2 ,2, 2],[2, 2, 2 ,2, 2],[2, 2, 2 ,2, 2],[2, 2, 2 ,2, 2],[2, 2, 2 ,2, 2]], device=device),\n",
    "                 torch.tensor([[2, 2, 2 ,2, 2],[2, 2, 2 ,2, 2],[2, 2, 2 ,2, 2],[2, 2, 2 ,2, 2],[2, 2, 2 ,2, 2]], device=device)]\n",
    "\n",
    "\n",
    "# CN_PN_RF, CN_PN_DN = create_weight_matrix(len(SA_CN_PN_RF)+len(RA_CN_PN_RF),num_output_neuron,connection_probability = 0.2, device = device)\n",
    "CN_PN_SA_RF, [CN_PN_SA_RF_step_height, CN_PN_SA_RF_step_width] = generate_weight(CN_PN_RF_set, pixel_h=SA_CN_PN_step_height, pixel_w=SA_CN_PN_step_width, step_size=1, device=device)\n",
    "CN_IN_SA_RF, [CN_IN_SA_RF_step_height, CN_IN_SA_RF_step_width] = generate_weight(CN_IN_RF_set, pixel_h=SA_CN_PN_step_height, pixel_w=SA_CN_PN_step_width, step_size=1, device=device)\n",
    "CN_SA_SD, [CN_SA_SD_step_height, CN_SA_SD_step_width] = generate_weight(\n",
    "    CN_SD_set, pixel_h=SA_CN_PN_step_height, pixel_w=SA_CN_PN_step_width, step_size=1, device=device)\n",
    "\n",
    "CN_PN_RA_RF, [CN_PN_RA_RF_step_height, CN_PN_RA_RF_step_width] = generate_weight(CN_PN_RF_RA_set, pixel_h=RA_CN_PN_step_height, pixel_w=RA_CN_PN_step_width, step_size=1, device=device)\n",
    "CN_IN_RA_RF, [CN_IN_RA_RF_step_height, CN_IN_RA_RF_step_width] = generate_weight(CN_IN_RF_RA_set, pixel_h=RA_CN_PN_step_height, pixel_w=RA_CN_PN_step_width, step_size=1, device=device)\n",
    "CN_RA_SD, [CN_RA_SD_step_height, CN_RA_SD_step_width] = generate_weight(\n",
    "    CN_SD__RA_set, pixel_h=RA_CN_PN_step_height, pixel_w=RA_CN_PN_step_width, step_size=1, device=device)\n",
    "\n",
    "CN_INtoPN_RF, CN_INtoPN_DN = create_weight_matrix(len(CN_IN_SA_RF), len(CN_PN_SA_RF), connection_probability=0.2, device=device)\n",
    "\n",
    "print(\"CN_PN_SA_RF shape: \", CN_PN_SA_RF.shape, \"CN_PN_SA_RF_step_height:\", CN_PN_SA_RF_step_height, \"CN_PN_SA_RF_step_width:\", CN_PN_SA_RF_step_width)\n",
    "print(\"CN_IN_SA_RF shape: \", CN_IN_SA_RF.shape, \"CN_IN_SA_RF_step_height:\", CN_IN_SA_RF_step_height, \"CN_IN_SA_RF_step_width:\", CN_IN_SA_RF_step_width)\n",
    "print(\"CN_PN_RA_RF shape: \", CN_PN_RA_RF.shape, \"CN_PN_RA_RF_step_height:\", CN_PN_RA_RF_step_height, \"CN_PN_RA_RF_step_width:\", CN_PN_RA_RF_step_width)\n",
    "print(\"CN_IN_RA_RF shape: \", CN_IN_RA_RF.shape, \"CN_IN_RA_RF_step_height:\", CN_IN_RA_RF_step_height, \"CN_IN_RA_RF_step_width:\", CN_IN_RA_RF_step_width)\n",
    "print(\"CN_INtoPN_RF shape: \", CN_INtoPN_RF.shape)\n",
    "############################################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_std_val = 4\n",
    "\n",
    "# Create Izhikevich layers\n",
    "SA_layer = IzhikevichLayer(0.02, 0.2, -65, 8, len(SA_RF),v_thres=30, a_decay=1.02, noise_std = noise_std_val, device=device)\n",
    "SA_CN_IN_layer = IzhikevichLayer(0.1, 0.2, -65, 6, len(SA_CN_IN_RF),v_thres=30, a_decay=1, noise_std = noise_std_val, device=device)\n",
    "SA_CN_PN_layer = IzhikevichLayer(0.1, 0.2, -65, 6, len(SA_CN_PN_RF),v_thres=30, a_decay=1, noise_std = noise_std_val, device=device)\n",
    "\n",
    "RA_layer = IzhikevichLayer(0.02, 0.2, -65, 8, len(RA_RF),v_thres=30, a_decay=1, noise_std = noise_std_val,device=device)\n",
    "RA_CN_IN_layer = IzhikevichLayer(0.1, 0.2, -65, 6, len(RA_CN_IN_RF),v_thres=30, a_decay=1, noise_std=noise_std_val, device=device)\n",
    "RA_CN_PN_layer = IzhikevichLayer(0.1, 0.2, -65, 6, len(RA_CN_PN_RF),v_thres=30, a_decay=1,noise_std = noise_std_val, device=device)\n",
    "\n",
    "SA_layers = [SA_layer, SA_CN_IN_layer, SA_CN_PN_layer]\n",
    "RA_layers = [RA_layer, RA_CN_IN_layer, RA_CN_PN_layer]\n",
    "\n",
    "SA_synapse = Synapse(SA_RF.to(device), device=device)\n",
    "SA_CN_IN_synapse = Synapse(SA_CN_IN_RF.to(device), delays=SA_CN_SD, device=device)\n",
    "SA_CN_PN_synapse = Synapse(SA_CN_PN_RF.to(device), delays=SA_CN_SD, device=device)\n",
    "SA_CN_INtoPN_synapse = Synapse(SA_INtoPN_RF.to(device), delays = SA_INtoPN_DN, tau_psp = 10, device = device)\n",
    "\n",
    "RA_synapse = Synapse(RA_RF.to(device), device=device)\n",
    "RA_CN_IN_synapse = Synapse(RA_CN_IN_RF.to(device), delays=RA_CN_SD, device=device)\n",
    "RA_CN_PN_synapse = Synapse(RA_CN_PN_RF.to(device), delays=RA_CN_SD, device=device)\n",
    "RA_CN_INtoPN_synapse = Synapse(RA_INtoPN_RF.to(device), delays = RA_INtoPN_DN, tau_psp = 10, device = device)\n",
    "\n",
    "SA_synapses = [SA_synapse, SA_CN_IN_synapse, SA_CN_PN_synapse, SA_CN_INtoPN_synapse]\n",
    "RA_synapses = [RA_synapse, RA_CN_IN_synapse, RA_CN_PN_synapse, RA_CN_INtoPN_synapse]\n",
    "\n",
    "# 3rd layer \n",
    "CN_IN_layer = IzhikevichLayer(0.1, 0.2, -65, 6, len(CN_IN_SA_RF), v_thres=30, a_decay=1, noise_std = noise_std_val, device=device)\n",
    "CN_PN_layer = IzhikevichLayer(0.1, 0.2, -65, 6, len(CN_PN_SA_RF),v_thres=30, a_decay=1, noise_std = noise_std_val, device=device)\n",
    "\n",
    "CN_IN_SA_synapse = Synapse(CN_IN_SA_RF.to(device), delays=CN_SA_SD, device=device)\n",
    "CN_PN_SA_synapse = Synapse(CN_PN_SA_RF.to(device), delays=CN_SA_SD, device=device)\n",
    "CN_IN_RA_synapse = Synapse(CN_IN_RA_RF.to(device), delays=CN_RA_SD, device=device)\n",
    "CN_PN_RA_synapse = Synapse(CN_PN_RA_RF.to(device), delays=CN_RA_SD, device=device)\n",
    "\n",
    "CN_INtoPN_synapse = Synapse(CN_INtoPN_RF.to(device), delays = CN_INtoPN_DN, tau_psp = 10, device = device)\n",
    "\n",
    "CN_layers = [CN_IN_layer, CN_PN_layer]\n",
    "CN_synapses = [CN_IN_SA_synapse, CN_PN_SA_synapse, CN_IN_RA_synapse, CN_PN_RA_synapse, CN_INtoPN_synapse]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-01 17:48:18.723390\n",
      "start feedforward\n",
      "Tactile sensor is open.\n",
      "FPS: 60.0\n",
      "procesing start\n",
      "0.05900287628173828\n",
      "processing end\n",
      "Tactile sensor is closed.\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())\n",
    "\n",
    "S = SNNModel([SA_layers, RA_layers, CN_layers], [SA_synapses, RA_synapses, CN_synapses], rf_sizes=rf_sizes, device = device)\n",
    "\n",
    "S.feedforward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def get_sensor_input(frame):\n",
    "    # Convert frame to grayscale\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Flip the frame vertically and rotate it counterclockwise by 90 degrees\n",
    "    flipped_frame = cv2.flip(gray_frame, 0)\n",
    "    rotated_frame = cv2.rotate(flipped_frame, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "    \n",
    "    # Resize the frame to desired dimensions\n",
    "    resized_frame = cv2.resize(rotated_frame, (48, 64))\n",
    "\n",
    "    # Apply Sobel edge detection\n",
    "    sobelx = cv2.Sobel(resized_frame, cv2.CV_64F, 1, 0, ksize=5)  # x-axis sobel edge detection\n",
    "    sobely = cv2.Sobel(resized_frame, cv2.CV_64F, 0, 1, ksize=5)  # y-axis sobel edge detection\n",
    "    sobel_frame = np.hypot(sobelx, sobely)  # combining both edges\n",
    "\n",
    "    # Normalize the result to 0-255\n",
    "    sobel_frame = sobel_frame / sobel_frame.max() * 255\n",
    "\n",
    "    return sobel_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_initial_sensor_settings(sensor):\n",
    "    desired_width = 320\n",
    "    desired_height = 240\n",
    "    sensor.set(cv2.CAP_PROP_FRAME_WIDTH, desired_width)\n",
    "    sensor.set(cv2.CAP_PROP_FRAME_HEIGHT, desired_height)\n",
    "\n",
    "    sensor.set(cv2.CAP_PROP_FPS, 60)\n",
    "    fps = sensor.get(cv2.CAP_PROP_FPS)\n",
    "    print(f\"FPS: {fps}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sensor_input(sensor, plot_image  = True):\n",
    "    ret, frame = sensor.read() \n",
    "\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    flipped_frame = cv2.flip(gray_frame, 0)\n",
    "    rotated_frame = cv2.rotate(flipped_frame, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "    resized_frame = cv2.resize(rotated_frame, (48, 64))\n",
    "\n",
    "    sobelx = cv2.Sobel(resized_frame, cv2.CV_64F, 1, 0, ksize=3)  # x-axis sobel edge detection # combining both edges  # x-axis sobel edge detection\n",
    "\n",
    "    if plot_image == True:\n",
    "        # cv2.imshow('frame', resized_frame)\n",
    "        cv2.imshow('sobel', sobelx)\n",
    "\n",
    "    return resized_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPS: 60.0\n",
      "Average Time per Frame: 0.009654074907302856 sec\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "set_initial_sensor_settings(cap)\n",
    "\n",
    "frames = []  # Frame을 저장할 리스트\n",
    "frame_count = 0  # 캡처한 Frame 수\n",
    "total_time = 0  # 측정 시간의 합계\n",
    "\n",
    "while True: \n",
    "    startT = time.time()\n",
    "\n",
    "    frame = get_sensor_input(cap, True)\n",
    "\n",
    "    frames.append(frame)  # 변경된 Frame을 리스트에 추가\n",
    "    frame_count += 1  # Frame 수 증가\n",
    "    \n",
    "    endT = time.time()\n",
    "    total_time += (endT - startT)  # 측정 시간 누적\n",
    "    # 10ms 기다리고 다음 프레임으로 전환, Esc누르면 while 강제 종료\n",
    "    if cv2.waitKey(1) == 27 or frame_count == 600:\n",
    "        break\n",
    "\n",
    "cap.release() \n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "avg_time = total_time / frame_count if frame_count else 0  # 평균 실행 시간 계산\n",
    "print(f\"Average Time per Frame: {avg_time} sec\")  # 평균 실행 시간 출력\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-CUDA10.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
